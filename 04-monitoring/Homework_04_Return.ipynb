{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc4139f-0fee-4687-b51d-37ef5551f961",
   "metadata": {},
   "source": [
    "## Homework: Evaluation and Monitoring\n",
    "\n",
    "In this homework, we'll evaluate the quality of our RAG system.\n",
    "\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668cf47-e213-4da4-9f80-631aac61ccaa",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Let's start by getting the dataset. We will use the data we generated in the module.\n",
    "\n",
    "In particular, we'll evaluate the quality of our RAG system\n",
    "with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv)\n",
    "\n",
    "\n",
    "Read it:\n",
    "\n",
    "```python\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "```\n",
    "\n",
    "We will use only the first 300 documents:\n",
    "\n",
    "\n",
    "```python\n",
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea36bfe6-1895-41c9-a048-c351143d2e07",
   "metadata": {},
   "source": [
    "# Intall the package below on first use.\n",
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af0a0e5-bfae-4399-99ac-ca030474d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data.\n",
    "import pandas as pd\n",
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b79a25e-7a5e-48be-9a8e-60919ba122cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first 300 documents.\n",
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cb853-680d-41e8-ae86-ffb150b406c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1670ac-3dbf-470c-bf5b-4a96164f6c19",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model\n",
    "\n",
    "Now, get the embeddings model `multi-qa-mpnet-base-dot-v1` from\n",
    "[the Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "> Note: this is not the same model as in HW3\n",
    "\n",
    "```bash\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "\n",
    "Create the embeddings for the first LLM answer:\n",
    "\n",
    "```python\n",
    "answer_llm = df.iloc[0].answer_llm\n",
    "```\n",
    "\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.42\n",
    "* -0.22\n",
    "* -0.02\n",
    "* 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c18a915-76c8-4e99-9578-123fe9ba19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding model.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585317cd-bbd6-4174-a0b8-0c20a4aea72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first LLM answer.\n",
    "answer_llm_0 = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365f2b9e-ac38-4423-bb0f-d455cb252e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings for the first LLM answer. \n",
    "v_llm_0 = embedding_model.encode(answer_llm_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86e5fbf-21c4-47b2-94bb-713a4fc03e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.42244655)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_llm_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e2ab6-a711-47b4-80d7-44766f436557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd95d04-b137-460c-8f61-0bfdb9c04b1b",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* 31.67\n",
    "* 41.67\n",
    "* 51.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1e06e8-d8a3-4127-b54f-6ab18995b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the answer_llm column.\n",
    "answer_llm_col = df.answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8b0783-2eda-4e70-a9b2-ca0b440bd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the answer_orig column.\n",
    "answer_orig_col = df.answer_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0509f4-c1ab-4267-b356-1fa8ad22ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings for answer_llm.\n",
    "v_answer_llm = embedding_model.encode(answer_llm_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88dd27e8-10b4-40d8-9414-e84557979e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings for answer_orig.\n",
    "v_answer_orig = embedding_model.encode(answer_orig_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a4d389-239f-40de-9104-ccc271c71580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dot product.\n",
    "evaluations = [llm.dot(orig) for llm, orig in zip(v_answer_llm, v_answer_orig)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f45927-a122-41d4-adde-c0af4e0b44a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(31.674309)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the 75th percentile of evaluations.\n",
    "import numpy as np\n",
    "np.percentile(evaluations, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff828bb-5192-40c7-9e56-232e7f4565ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0c4f067-e1a5-4974-bdaa-4d14c2fea3eb",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we \n",
    "\n",
    "* Compute the norm of a vector\n",
    "* Divide each element by this norm\n",
    "\n",
    "So, for vector `v`, it'll be `v / ||v||`\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "```python\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```\n",
    "\n",
    "Let's put it into a function and then compute dot product \n",
    "between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?\n",
    "\n",
    "* 0.63\n",
    "* 0.73\n",
    "* 0.83\n",
    "* 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2322ffa1-1426-4315-bd83-bc208d9a2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalise a vector.\n",
    "def norm_vec(the_vec):\n",
    "    norm_the_vec = [vec / np.sqrt((vec * vec).sum()) for vec in the_vec]\n",
    "    return norm_the_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2749b5dc-3cfa-4f7f-88e3-431cbc24e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise answer_llm vector.\n",
    "v_norm_answer_llm = norm_vec(v_answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29439d32-37fa-4c07-b8d0-aadee8e7d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise answer_orig vector.\n",
    "v_norm_answer_orig = norm_vec(v_answer_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d920153d-e36d-4349-9b85-15294e3dd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dot product of the normalised vectors.\n",
    "cosine_similarity = [n_llm.dot(n_orig) for n_llm, n_orig in zip(v_norm_answer_llm, v_norm_answer_orig)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f00b207-b704-4a22-8515-89ddb3409901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.8362349)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the 75th percentile of cosine_similarity.\n",
    "np.percentile(cosine_similarity, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613b9e6-b273-4b9a-9b7a-ea4e8092f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c08ada02-0e85-4702-ae68-17ea7f9b7f2c",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "Now we will explore an alternative metric - the ROUGE score.  \n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "```bash\n",
    "pip install rouge\n",
    "```\n",
    "\n",
    "(The latest version at the moment of writing is `1.0.1`)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "\n",
    "```\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "What's the F score for `rouge-1`?\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88673afc-9a45-492b-b7ba-39a7f4fe97ee",
   "metadata": {},
   "source": [
    "# Intall the package below on first use.\n",
    "pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ee17bf-c77b-473a-9907-53152e3e7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rouge_scorer.\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cfc6b61-425b-43e7-8c10-7f307954d569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     Yes, all sessions are recorded, so if you miss...\n",
       "answer_orig    Everything is recorded, so you wonâ€™t miss anyt...\n",
       "document                                                5170565b\n",
       "question                    Are sessions recorded if I miss one?\n",
       "course                                 machine-learning-zoomcamp\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the document at index 10.\n",
    "doc_10 = df.iloc[10]\n",
    "doc_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86555a1b-c9b3-4c12-9e47-4d8f294d25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select answer_llm at index 10 document.\n",
    "answer_llm_10 = doc_10.answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360fd9f6-ae1a-4cdd-8fd8-f2335e0bef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select answer_orig at index 10 document.\n",
    "answer_orig_10 = doc_10.answer_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91d384b1-15a4-4345-896e-bea1edc86529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the rogue scores of document at index 10.\n",
    "scores_10 = rouge_scorer.get_scores(answer_llm_10, answer_orig_10)[0]\n",
    "scores_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec7d4bc-5e8c-4b03-81f4-1787933ef2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454544954545456"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the rogue-1 f score of document at index 10.\n",
    "scores_10_1_f = scores_10['rouge-1']['f']\n",
    "scores_10_1_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d3053-45e3-4211-aaaa-5b205419e9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87acc370-e34f-4069-9605-97c18e1bed5a",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f3c67a3-b9a0-41bc-b41a-4fe31a2e589c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21621621121621637"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the rogue-2 f score of document at index 10.\n",
    "scores_10_2_f = scores_10['rouge-2']['f']\n",
    "scores_10_2_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81312800-9bdc-4de9-84df-02c48aea1b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.393939388939394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the rogue-l f score of document at index 10.\n",
    "scores_10_l_f = scores_10['rouge-l']['f']\n",
    "scores_10_l_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7aafadc-a405-43ef-9b86-6d3872c38201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490034990035496"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average of the returned scores.\n",
    "scores_10_avg = (scores_10_1_f + scores_10_2_f + scores_10_l_f) / 3\n",
    "scores_10_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da387549-4393-46ef-982d-ec540c3407a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3dfa23-f246-4022-9e06-883e9bffe232",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records\n",
    "\n",
    "```python\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "```\n",
    "\n",
    "And create a dataframe from them\n",
    "\n",
    "What's the avgerage `rouge_2` across all the records?\n",
    "\n",
    "- 0.10\n",
    "- 0.20\n",
    "- 0.30\n",
    "- 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ffed05d-98f3-4e0d-9a30-2001a889d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the score for all the records.\n",
    "rouge_score = [rouge_scorer.get_scores(r_llm, r_orig)[0] for r_llm, r_orig in zip(answer_llm_col, answer_orig_col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99453000-3b12-4f48-8408-da5d865f1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from rouge_score.\n",
    "df_rouge_score = pd.DataFrame(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45bfbb3f-3166-4b50-b08d-f70eb6c85f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'r': 0.017543859649122806, 'p': 0.07142857142...\n",
       "1      {'r': 0.03508771929824561, 'p': 0.133333333333...\n",
       "2      {'r': 0.14035087719298245, 'p': 0.242424242424...\n",
       "3      {'r': 0.03508771929824561, 'p': 0.071428571428...\n",
       "4      {'r': 0.07017543859649122, 'p': 0.022346368715...\n",
       "                             ...                        \n",
       "295    {'r': 0.559322033898305, 'p': 0.52380952380952...\n",
       "296    {'r': 0.5423728813559322, 'p': 0.4, 'f': 0.460...\n",
       "297    {'r': 0.5932203389830508, 'p': 0.5384615384615...\n",
       "298    {'r': 0.13559322033898305, 'p': 0.129032258064...\n",
       "299    {'r': 0.01694915254237288, 'p': 0.038461538461...\n",
       "Name: rouge-2, Length: 300, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the rouge-2 column in df_rouge_score. \n",
    "df_rouge_score['rouge-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c6a95f9-0562-4d99-b3c4-a84854d3073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 0.017543859649122806,\n",
       " 'p': 0.07142857142857142,\n",
       " 'f': 0.028169010918468917}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first item in rouge-2 column. \n",
    "df_rouge_score['rouge-2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3311ce12-f978-4f9c-b6b5-0e70ce6daad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028169010918468917"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first f score of rouge-2. \n",
    "df_rouge_score['rouge-2'][0]['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c52ccb3-1223-4f95-8eb9-70ccd1da6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of rouge-2.\n",
    "rouge_2 = np.mean([s_r2['f'] for s_r2 in df_rouge_score['rouge-2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "391ab61c-fe92-4a2c-aaa6-aa16cd3f776a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.20696501983423318)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bfd62-0483-44b9-b7ed-71cf039060f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
